

# ğŸš€ Pipeline de IngestÃ£o de Dados - Lighthouse Checkpoint 2

## 1. VisÃ£o Geral do Projeto

Este projeto implementa uma pipeline de ingestÃ£o de dados robusta e eficiente, desenvolvida como parte do desafio Lighthouse Checkpoint 2 da Indicium.  
O objetivo principal Ã© extrair dados de duas fontes distintas â€” um banco de dados relacional (MSSQL) e uma API REST â€” e carregÃ¡-los em um ambiente Databricks Lakehouse, utilizando o formato Delta Lake para garantir otimizaÃ§Ã£o, escalabilidade e conformidade com boas prÃ¡ticas de engenharia de dados.

A soluÃ§Ã£o foi projetada com foco em modularidade, reutilizaÃ§Ã£o, clareza e manutenibilidade do cÃ³digo. Segue princÃ­pios sÃ³lidos de engenharia de dados, como:

- Gerenciamento seguro de credenciais
- Tratamento abrangente de erros
- Garantia de idempotÃªncia na execuÃ§Ã£o da pipeline

---

## 2. Arquitetura da SoluÃ§Ã£o

A orquestraÃ§Ã£o da pipeline Ã© feita com **Meltano**, uma plataforma ELT open-source, conteinerizada com **Docker** para garantir consistÃªncia de ambiente. Os dados extraÃ­dos sÃ£o temporariamente salvos em arquivos Parquet antes de serem carregados no Databricks como tabelas Delta Lake.

### 2.1. Componentes TÃ©cnicos

- **Meltano (Orquestrador ELT):**
  - `tap-mssql`: Extrator que se conecta ao banco MSSQL.
  - `tap-rest-api-msdk`: Extrator para API REST, com paginaÃ§Ã£o `offset` e `limit`. Streams:
    - `SalesOrderHeader`
    - `SalesOrderDetail`
    - `PurchaseOrderHeader`
    - `PurchaseOrderDetail`
  - `target-parquet`: Loader que salva os dados em arquivos `.parquet` organizados por fonte:
    - `target-parquet-sqlserver`
    - `target-parquet-api`

- **Docker:** Utilizado para conteinerizar o ambiente Meltano e todas as dependÃªncias (Databricks CLI, Python, pacotes de sistema). O `Dockerfile` Ã© baseado na imagem `meltano/meltano:latest-python3.11`.

- **Databricks CLI (v2):** ResponsÃ¡vel pelo upload dos arquivos Parquet para o Lakehouse (via DBFS ou Unity Catalog).

- **Databricks Lakehouse (Delta Lake):** Destino final dos dados. Vantagens:
  - TransaÃ§Ãµes ACID
  - Controle de schema (enforcement & evolution)
  - Performance otimizada para anÃ¡lise
  - Alta escalabilidade

---

## 3. Requisitos e PrÃ©-requisitos

Para executar este projeto, vocÃª precisarÃ¡ de:

- **Docker Desktop** (4.x+) ou **Docker Engine**
- **Git**
- **Acesso ao Databricks:**
  - URL do workspace
  - Personal Access Token (PAT)
- **Acesso ao banco MSSQL:**
  - Host, porta, usuÃ¡rio, senha e nome do banco
- **Acesso Ã  API:**
  - URL base, usuÃ¡rio e senha (basic auth)

---

## 4. ConfiguraÃ§Ã£o do Ambiente Local

### 4.1. Clonar o repositÃ³rio

```bash
git clone <URL_DO_REPOSITORIO_PRIVADO>
cd lighthouse-ingestion-pipeline
````

> **Importante:** Conceda acesso ao seu repositÃ³rio para os avaliadores antes do prazo final.

### 4.2. VariÃ¡veis de Ambiente

Crie um arquivo `.env` com base no `.env.save`:

```ini
# MSSQL
TAP_MSSQL_HOST=your_mssql_host
TAP_MSSQL_PORT=your_mssql_port
TAP_MSSQL_USER=your_mssql_user
TAP_MSSQL_PASSWORD=your_mssql_password
TAP_MSSQL_DATABASE=your_mssql_database

# API
API_HOST=your_api_base_url
API_USER=your_api_username
API_PASSWORD=your_api_password

# Databricks
DATABRICKS_HOST=your_databricks_workspace_url
DATABRICKS_TOKEN=your_databricks_token
```

> âš ï¸ **Nunca faÃ§a commit de dados sensÃ­veis.**

### 4.3. Estrutura do Projeto

```
.
â”œâ”€â”€ extract/                 
â”œâ”€â”€ load/
â”œâ”€â”€ plugins/
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env.save
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ entrypoint.sh
â”œâ”€â”€ meltano.yml
â””â”€â”€ requirements.txt
```

---

## 5. ExecuÃ§Ã£o da Pipeline

### 5.1. Construir a Imagem Docker

```bash
docker build -t lighthouse-ingestion-pipeline .
```

### 5.2. Executar o ContÃªiner

```bash
docker run \
  --env-file .env \
  lighthouse-ingestion-pipeline
```

O script `entrypoint.sh` serÃ¡ executado automaticamente.

#### 5.2.1. O que o `entrypoint.sh` faz:

1. Executa o pipeline com Meltano:

```bash
meltano run tap-mssql target-parquet-sqlserver tap-rest-api-msdk target-parquet-api
```

2. Realiza o upload dos arquivos Parquet para o Databricks:

```bash
# Exemplo com DBFS
databricks fs cp output/docker_elt/sqlserver/ dbfs:/<caminho>/sqlserver/ --recursive --overwrite
databricks fs cp output/docker_elt/api/ dbfs:/<caminho>/api/ --recursive --overwrite
```

> Ajuste os comandos de acordo com seu ambiente Databricks (DBFS vs Unity Catalog).

---

### 5.3. ValidaÃ§Ã£o

1. Acesse seu workspace no Databricks.
2. Verifique os arquivos nos caminhos corretos.
3. Crie ou valide as tabelas Delta Lake.
4. Execute queries SQL para testar integridade dos dados.

---

## 6. Contato

Em caso de dÃºvidas ou sugestÃµes, entre em contato:

**Nome da equipe**:*Alexandre R.Silva Filho*
**Email:** **alexandre.filho@indicium.tech** 
