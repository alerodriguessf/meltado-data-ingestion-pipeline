
# ğŸš€ Pipeline de IngestÃ£o de Dados - Lighthouse Checkpoint 2

## 1. VisÃ£o Geral do Projeto

Este projeto implementa uma pipeline de ingestÃ£o de dados robusta e eficiente, desenvolvida como parte do desafio Lighthouse Checkpoint 2 da Indicium.

O objetivo principal Ã© extrair dados de duas fontes distintas â€” um banco de dados relacional (MSSQL) e uma API REST â€” e carregÃ¡-los em um ambiente Databricks Lakehouse, utilizando o formato Delta Lake para garantir otimizaÃ§Ã£o, escalabilidade e conformidade com boas prÃ¡ticas de engenharia de dados.

A soluÃ§Ã£o foi projetada com foco em modularidade, reutilizaÃ§Ã£o, clareza e manutenibilidade do cÃ³digo. Segue princÃ­pios sÃ³lidos de engenharia de dados, como:

- Gerenciamento seguro de credenciais  
- Tratamento abrangente de erros  
- Garantia de idempotÃªncia na execuÃ§Ã£o da pipeline

---

## 2. Arquitetura da SoluÃ§Ã£o

A orquestraÃ§Ã£o da pipeline Ã© feita com Meltano, uma plataforma ELT open-source, conteinerizada com Docker para garantir consistÃªncia de ambiente. Os dados extraÃ­dos sÃ£o temporariamente salvos em arquivos Parquet antes de serem carregados no Databricks como tabelas Delta Lake.

### 2.1. Componentes TÃ©cnicos

**Meltano (Orquestrador ELT):**

- `tap-mssql`: Extrator que se conecta ao banco MSSQL.  
- `tap-rest-api-msdk`: Extrator para API REST, com paginaÃ§Ã£o offset e limit. Streams:  
  - SalesOrderHeader  
  - SalesOrderDetail  
  - PurchaseOrderHeader  
  - PurchaseOrderDetail  
- `target-parquet`: Loader que salva os dados em arquivos `.parquet` organizados por fonte:
  - `target-parquet-sqlserver`
  - `target-parquet-api`

**Docker:** Utilizado para conteinerizar o ambiente Meltano e todas as dependÃªncias (Databricks CLI, Python, pacotes de sistema). O `Dockerfile` Ã© baseado na imagem `meltano/meltano:latest-python3.11`.

**Databricks CLI (v2):** ResponsÃ¡vel pelo upload dos arquivos Parquet para o Lakehouse (via DBFS ou Unity Catalog).

**Databricks Lakehouse (Delta Lake):** Destino final dos dados. Vantagens:
- TransaÃ§Ãµes ACID  
- Controle de schema (enforcement & evolution)  
- Performance otimizada para anÃ¡lise  
- Alta escalabilidade  

---

## 3. Requisitos e PrÃ©-requisitos

Para executar este projeto, vocÃª precisarÃ¡ de:

- Docker Desktop (4.x+) ou Docker Engine  
- Git  
- Acesso ao Databricks:  
  - URL do workspace  
  - Personal Access Token (PAT)  
- Acesso ao banco MSSQL:  
  - Host, porta, usuÃ¡rio, senha e nome do banco  
- Acesso Ã  API:  
  - URL base, usuÃ¡rio e senha (basic auth)  

---

## 4. ConfiguraÃ§Ã£o do Ambiente Local

### 4.1. Clonar o repositÃ³rio

```bash
git clone <URL_DO_REPOSITORIO_PRIVADO>
cd lighthouse-ingestion-pipeline
````

> âš ï¸ Importante: Conceda acesso ao seu repositÃ³rio para os avaliadores antes do prazo final.

### 4.2. VariÃ¡veis de Ambiente

Crie um arquivo `.env` com base no `.env.save`:

```env
# MSSQL
TAP_MSSQL_HOST=your_mssql_host
TAP_MSSQL_PORT=your_mssql_port
TAP_MSSQL_USER=your_mssql_user
TAP_MSSQL_PASSWORD=your_mssql_password
TAP_MSSQL_DATABASE=your_mssql_database

# API
API_HOST=your_api_base_url
API_USER=your_api_username
API_PASSWORD=your_api_password

# Databricks
DATABRICKS_HOST=your_databricks_workspace_url
DATABRICKS_TOKEN=your_databricks_token
```

> âš ï¸ Nunca faÃ§a commit de dados sensÃ­veis.

### 4.3. Estrutura do Projeto

```
.
â”œâ”€â”€ extract/                 
â”œâ”€â”€ load/
â”œâ”€â”€ plugins/
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env.save
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ entrypoint.sh
â”œâ”€â”€ meltano.yml
â””â”€â”€ requirements.txt
```

---

## 5. ExecuÃ§Ã£o da Pipeline

### 5.1. Construir a Imagem Docker

```bash
docker build -t lighthouse-ingestion-pipeline .
```

### 5.2. Executar o ContÃªiner

```bash
docker run \
  --env-file .env \
  lighthouse-ingestion-pipeline
```

O script `entrypoint.sh` serÃ¡ executado automaticamente.

### 5.2.1. O que o `entrypoint.sh` faz:

* Executa o pipeline com Meltano:

  ```bash
  meltano run tap-mssql target-parquet-sqlserver tap-rest-api-msdk target-parquet-api
  ```
* Realiza o upload dos arquivos Parquet para o Databricks:

  ```bash
  databricks fs cp output/docker_elt/sqlserver/ dbfs:/<caminho>/sqlserver/ --recursive --overwrite
  databricks fs cp output/docker_elt/api/ dbfs:/<caminho>/api/ --recursive --overwrite
  ```

> âš™ï¸ Ajuste os caminhos conforme seu ambiente Databricks (DBFS vs Unity Catalog).

---

## 6. ConversÃ£o Manual em Tabelas Delta

ApÃ³s o upload dos arquivos Parquet para o Databricks, utilizamos **notebooks dedicados** para realizar a conversÃ£o manual das partiÃ§Ãµes Parquet em **tabelas Delta Lake** no nÃ­vel *bronze*.

Esses notebooks foram desenvolvidos de forma modular e possuem as seguintes responsabilidades:

* Ler recursivamente os arquivos `.parquet` em subpastas
* Inferir o schema dos dados automaticamente
* Criar tabelas Delta gerenciadas com nomes padronizados como:

  * `raw_api_<nome>_db`
  * `raw_sqlserver_<nome>_db`

### Notebooks disponÃ­veis:

* `delta_conversion_api_checkpoint2_alexandrersf`: converte todos os arquivos da API em tabelas Delta no Unity Catalog
* `delta_conversion_sqlserver_checkpoint2_alexandrersf`: mesma lÃ³gica aplicada aos arquivos provenientes do SQL Server

Esses scripts estÃ£o localizados na pasta `/Workspace/Users/<seu_usuario_databricks>/notebooks/` e podem ser executados diretamente no Databricks.

> âœ… O processo Ã© automatizado e garante que cada partiÃ§Ã£o gere uma tabela individual.
> âœ… Todos os nomes seguem um padrÃ£o que inclui a fonte de origem, facilitando o rastreio e a auditoria.

---

## 7. ValidaÃ§Ã£o

* Acesse seu workspace no Databricks
* Verifique se as tabelas Delta foram criadas no catÃ¡logo correto
* Execute queries SQL para validar a integridade dos dados
* Confirme se a nomenclatura das tabelas segue o padrÃ£o definido

---

## 8. Contato

**Equipe:** Alexandre R. Silva Filho
ğŸ“§ **Email:** [alexandre.filho@indicium.tech](mailto:alexandre.filho@indicium.tech)

---

## ğŸ“Œ Sobre

Desafio de infraestrutura e ingestÃ£o de dados do programa Lighthouse - Indicium
