
# ğŸš€ Pipeline de IngestÃ£o de Dados â€” Lighthouse Checkpoint 2

## 1. VisÃ£o Geral do Projeto

Este projeto foi desenvolvido como parte do desafio **Lighthouse Checkpoint 2** da Indicium, com o objetivo de implementar uma **pipeline de ingestÃ£o de dados confiÃ¡vel, modular e escalÃ¡vel**, conectando mÃºltiplas fontes e carregando os dados em um ambiente analÃ­tico moderno: o **Databricks Lakehouse**.

A soluÃ§Ã£o realiza a ingestÃ£o de dados a partir de:

* Um **banco de dados relacional** (MSSQL)
* Uma **API REST**

Os dados extraÃ­dos sÃ£o temporariamente armazenados no formato **Parquet**, e posteriormente convertidos manualmente para o formato **Delta Lake**, garantindo performance, integridade e governanÃ§a.

A arquitetura foi construÃ­da com foco em:

* Clareza, manutenibilidade e modularidade
* Boas prÃ¡ticas de engenharia de dados
* SeguranÃ§a de credenciais
* Portabilidade do ambiente com Docker

---

## 2. Arquitetura da SoluÃ§Ã£o

A pipeline extrai os dados a partir do **Meltano**, uma ferramenta ELT open-source que permite controle granular das etapas de extraÃ§Ã£o e carga. Todo o projeto Ã© conteinerizado com **Docker**, assegurando reprodutibilidade do ambiente de desenvolvimento e execuÃ§Ã£o.

ApÃ³s a extraÃ§Ã£o, os dados sÃ£o organizados em arquivos `.parquet` e transferidos para o **Databricks**, onde sÃ£o convertidos em **tabelas Delta Lake**, com suporte a:

* TransaÃ§Ãµes ACID
* Schema enforcement & evolution
* OtimizaÃ§Ãµes de leitura e escrita
* Alta escalabilidade

### ğŸ”§ Componentes TÃ©cnicos

| Componente           | Papel na Pipeline                                                                                     |
| -------------------- | ----------------------------------------------------------------------------------------------------- |
| `tap-mssql`          | Extrai dados do banco SQL Server utilizando autenticaÃ§Ã£o por usuÃ¡rio/senha                            |
| `tap-rest-api-msdk`  | Conecta-se Ã  API REST utilizando autenticaÃ§Ã£o bÃ¡sica (Basic Auth), acessando os endpoints disponÃ­veis |
| `target-parquet`     | Converte os dados extraÃ­dos em arquivos `.parquet` organizados por origem                             |
| `Docker`             | Cria ambiente reprodutÃ­vel com Meltano, Databricks CLI e dependÃªncias Python                          |
| `Databricks CLI`     | Realiza o upload dos arquivos `.parquet` para o Databricks via terminal                               |
| Notebooks auxiliares | Realizam a conversÃ£o dos arquivos `.parquet` em tabelas Delta, de forma controlada e modular          |

---

## 3. Requisitos e PrÃ©-Requisitos

Para executar a pipeline localmente, sÃ£o necessÃ¡rias as seguintes ferramentas e acessos:

### ğŸ§° Ferramentas

* [Docker Desktop](https://www.docker.com/products/docker-desktop/) (v4.x+)
* [Git](https://git-scm.com/)
* [Databricks CLI](https://docs.databricks.com/dev-tools/cli/index.html) (prÃ©-instalado no container)

### ğŸ” Acessos NecessÃ¡rios

* URL e credenciais do banco **MSSQL**
* URL e credenciais da **API REST**
* URL do workspace **Databricks** e um **Token PAT** com permissÃµes de escrita

---

## 4. ConfiguraÃ§Ã£o do Ambiente

### 4.1 Clonar o RepositÃ³rio

```bash
git clone https://github.com/alerodriguessf/lighthouse_desafio02_alexandrersf
cd lighthouse_desafio02_alexandrersf
```

### 4.2 Configurar VariÃ¡veis de Ambiente

Crie um arquivo `.env` com suas credenciais:

```env
# MSSQL
TAP_MSSQL_HOST=your_mssql_host
TAP_MSSQL_PORT=1433
TAP_MSSQL_USER=your_user
TAP_MSSQL_PASSWORD=your_password
TAP_MSSQL_DATABASE=AdventureWorks2022

# API
API_HOST=https://your-api-url.com
API_USER=your_api_user
API_PASSWORD=your_api_password

# DATABRICKS
DATABRICKS_HOST=https://your-databricks-instance.cloud.databricks.com
DATABRICKS_TOKEN=your_pat_token
```

> ğŸ” **Importante**: NÃ£o versionar este arquivo com Git.

---

## 5. Estrutura do Projeto

```
.
â”œâ”€â”€ extract/                            # ExtraÃ§Ã£o via Meltano
â”œâ”€â”€ load/                               # Carga via Meltano
â”œâ”€â”€ plugins/                            # Plugins Meltano
â”œâ”€â”€ scripts_aux/                        # Scripts auxiliares para discovery e Delta
â”‚   â”œâ”€â”€ discovery_api_aw_checkpoint2.ipynb
â”‚   â”œâ”€â”€ delta conversion_api_checkpoint2_alexandrersf (1).ipynb
â”‚   â””â”€â”€ delta conversion_sqlserver_checkpoint2_alexandrersf.ipynb
â”œâ”€â”€ Dockerfile                          # Build da imagem com Meltano + CLI Databricks
â”œâ”€â”€ entrypoint.sh                       # Script de orquestraÃ§Ã£o do pipeline
â”œâ”€â”€ .env.save                           # Modelo de exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore
â”œâ”€â”€ meltano.yml                         # ConfiguraÃ§Ã£o principal do projeto Meltano
â”œâ”€â”€ requirements.txt                    # DependÃªncias Python
â””â”€â”€ README.md                           # Este arquivo
```

---

## 6. ExecuÃ§Ã£o da Pipeline

### 6.1 Construir a Imagem Docker

```bash
docker build -t lighthouse-ingestion-pipeline .
```

### 6.2 Executar o ContÃªiner

```bash
docker run --env-file .env lighthouse-ingestion-pipeline
```

Este comando aciona o `entrypoint.sh`, que:

1. Executa a extraÃ§Ã£o dos dados via Meltano:

```bash
meltano run tap-mssql target-parquet-sqlserver tap-rest-api-msdk target-parquet-api
```

2. Realiza o upload dos arquivos `.parquet` para o Databricks:

```bash
databricks fs cp output/docker_elt/sqlserver/ dbfs:/mnt/<caminho>/sqlserver/ --recursive --overwrite
databricks fs cp output/docker_elt/api/ dbfs:/mnt/<caminho>/api/ --recursive --overwrite
```

---

## 7. Scripts Auxiliares (`scripts_aux/`)

Para facilitar testes, validaÃ§Ãµes e modularizar a etapa final da ingestÃ£o, foram incluÃ­dos trÃªs notebooks:

| Notebook                                                    | FunÃ§Ã£o                                                           |
| ----------------------------------------------------------- | ---------------------------------------------------------------- |
| `discovery_api_aw_checkpoint2.ipynb`                        | ValidaÃ§Ã£o e visualizaÃ§Ã£o dos dados expostos pela API REST        |
| `delta conversion_api_checkpoint2_alexandrersf (1).ipynb`   | CriaÃ§Ã£o de tabelas Delta a partir dos arquivos `.parquet` da API |
| `delta conversion_sqlserver_checkpoint2_alexandrersf.ipynb` | ConversÃ£o dos dados SQL Server para Delta Lake                   |

Esses notebooks foram essenciais para garantir:

* Flexibilidade diante da instabilidade da API
* ModularizaÃ§Ã£o das etapas de ingestÃ£o
* Controle e auditabilidade do processo

As tabelas Delta criadas seguem o padrÃ£o de nomenclatura:

```
raw_api_<tabela>_db
raw_sqlserver_<tabela>_db
```

---

## 8. Contato

**Autor:** Alexandre R.Silva Filho
ğŸ“§ [alexandre.filho@indicium.tech](mailto:alexandre.filho@indicium.tech)
ğŸ”— [LinkedIn](https://www.linkedin.com/in/alerodriguessf/)

