

# üöÄ Pipeline de Ingest√£o de Dados - Lighthouse Checkpoint 2

## 1. Vis√£o Geral do Projeto

Este projeto implementa uma pipeline de ingest√£o de dados robusta e eficiente, desenvolvida como parte do desafio **Lighthouse Checkpoint 2** da Indicium.

O objetivo principal √© extrair dados de duas fontes distintas ‚Äî um banco de dados relacional (**MSSQL**) e uma **API REST** ‚Äî e carreg√°-los em um ambiente **Databricks Lakehouse**, utilizando o formato **Delta Lake** para garantir otimiza√ß√£o, escalabilidade e conformidade com boas pr√°ticas de engenharia de dados.

A solu√ß√£o foi projetada com foco em **modularidade**, **reutiliza√ß√£o**, **clareza** e **manutenibilidade do c√≥digo**, seguindo princ√≠pios s√≥lidos de engenharia de dados, como:

* Gerenciamento seguro de credenciais
* Garantia de idempot√™ncia na execu√ß√£o da pipeline

---

## 2. Arquitetura da Solu√ß√£o

A orquestra√ß√£o da pipeline √© feita com **Meltano**, uma plataforma ELT open-source, conteinerizada com **Docker** para garantir consist√™ncia de ambiente. Os dados extra√≠dos s√£o temporariamente salvos em arquivos `.parquet` antes de serem carregados no **Databricks**.

### 2.1 Componentes T√©cnicos

#### üîß Meltano (Extra√ß√£o de dados):

* **tap-mssql**: Extrator que se conecta ao banco MSSQL
* **tap-rest-api-msdk**: Extrator para API REST, com pagina√ß√£o por offset e limit

  * `SalesOrderHeader`
  * `SalesOrderDetail`
  * `PurchaseOrderHeader`
  * `PurchaseOrderDetail`
* **target-parquet**: Loader que salva os dados em arquivos `.parquet`, organizados por fonte:

  * `target-parquet-sqlserver`
  * `target-parquet-api`

#### üê≥ Docker:

* Utilizado para conteinerizar o ambiente Meltano e todas as depend√™ncias (Databricks CLI, Python, bibliotecas nativas)
* Imagem base: `meltano/meltano:latest-python3.11`

#### ‚òÅÔ∏è Databricks Lakehouse (Delta Lake):

* **Destino final** dos dados, com suporte a:

  * Transa√ß√µes ACID
  * Controle de schema (enforcement & evolution)
  * Performance otimizada
  * Escalabilidade horizontal

---

## 3. Requisitos e Pr√©-requisitos

Para executar o projeto localmente, √© necess√°rio:

* Docker Desktop (v4.x+) ou Docker Engine
* Git
* Acesso ao Databricks (workspace URL e PAT)
* Acesso ao banco MSSQL
* Acesso √† API REST

---

## 4. Configura√ß√£o do Ambiente Local

### 4.1 Clonar o Reposit√≥rio

```bash
git clone <https://github.com/alerodriguessf/lighthouse_desafio02_alexandrersf>

```

### 4.2 Vari√°veis de Ambiente

Crie um arquivo `.env` com base nas suas credenciais

```bash
# MSSQL
TAP_MSSQL_HOST=your_mssql_host
TAP_MSSQL_PORT=your_mssql_port
TAP_MSSQL_USER=your_mssql_user
TAP_MSSQL_PASSWORD=your_mssql_password
TAP_MSSQL_DATABASE=your_mssql_database

# API
API_HOST=your_api_base_url
API_USER=your_api_username
API_PASSWORD=your_api_password

# Databricks
DATABRICKS_HOST=your_databricks_workspace_url
DATABRICKS_TOKEN=your_databricks_token
```

> ‚ö†Ô∏è Nunca fa√ßa commit de credenciais ou tokens.

---

## 5. Estrutura do Projeto

```
.
‚îú‚îÄ‚îÄ extract/                            # Extra√ß√£o de dados via Meltano
‚îú‚îÄ‚îÄ load/                               # Load de dados via Meltano
‚îú‚îÄ‚îÄ plugins/                            # Plugins Meltano (tap/target)
‚îú‚îÄ‚îÄ scripts_aux/                        # Scripts auxiliares (discovery e Delta conversion)
‚îÇ   ‚îú‚îÄ‚îÄ discovery_api_aw_checkpoint2.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ delta conversion_api_checkpoint2_alexandrersf (1).ipynb
‚îÇ   ‚îî‚îÄ‚îÄ delta conversion_sqlserver_checkpoint2_alexandrersf.ipynb
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .env.save
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ entrypoint.sh
‚îú‚îÄ‚îÄ meltano.yml
‚îî‚îÄ‚îÄ requirements.txt
```

---

## 6. Execu√ß√£o da Pipeline

### 6.1 Construir a Imagem Docker

```bash
docker build -t lighthouse-ingestion-pipeline .
```

### 6.2 Executar o Cont√™iner

```bash
docker run \
  --env-file .env \
  lighthouse-ingestion-pipeline
```

> O script `entrypoint.sh` ser√° executado automaticamente.

### 6.2.1 O que o entrypoint.sh faz:

1. Executa o pipeline com Meltano:

```bash
meltano run tap-mssql target-parquet-sqlserver tap-rest-api-msdk target-parquet-api
```

2. Realiza o upload dos arquivos Parquet para o Databricks:

```bash
databricks fs cp output/docker_elt/sqlserver/ dbfs:/<caminho>/sqlserver/ --recursive --overwrite
databricks fs cp output/docker_elt/api/ dbfs:/<caminho>/api/ --recursive --overwrite
```

> Ajuste os caminhos conforme sua estrutura (`DBFS` ou `Unity Catalog`).

---

## 7. Valida√ß√£o

* Acesse o seu workspace no **Databricks**
* Valide a exist√™ncia dos arquivos Parquet
* Execute os notebooks de convers√£o de Parquet para Delta
* Verifique as tabelas criadas com o prefixo:

  * `raw_api_<nome>_db`
  * `raw_sqlserver_<nome>_db`

---

## 8. Scripts Auxiliares (`scripts_aux/`)

Al√©m da pipeline Meltano, o reposit√≥rio conta com **scripts auxiliares** importantes para testes e ingest√£o manual.

### üìÇ Conte√∫do da Pasta

```
scripts_aux/
‚îú‚îÄ‚îÄ discovery_api_aw_checkpoint2.ipynb                  # Testa endpoints da API REST
‚îú‚îÄ‚îÄ delta conversion_api_checkpoint2_alexandrersf (1).ipynb     # Cria tabelas Delta a partir dos dados da API
‚îî‚îÄ‚îÄ delta conversion_sqlserver_checkpoint2_alexandrersf.ipynb   # Cria tabelas Delta a partir dos dados SQL Server
```

### üß™ Utiliza√ß√£o

* `discovery_api_aw_checkpoint2.ipynb`: Garante que os endpoints da API est√£o respondendo corretamente
* `delta conversion_api_checkpoint2_alexandrersf (1).ipynb`: Busca os arquivos `.parquet` da API e os converte em tabelas Delta individuais
* `delta conversion_sqlserver_checkpoint2_alexandrersf.ipynb`: Idem ao anterior, mas para arquivos extra√≠dos do SQL Server

> ‚úÖ Os nomes das tabelas seguem o padr√£o: `raw_<fonte>_<nome>_db`, garantindo organiza√ß√£o e rastreabilidade.

---

## 9. Contato

**Nome da equipe:** Alexandre R.Silva Filho

**Email:** [alexandre.filho@indicium.tech](mailto:alexandre.filho@indicium.tech)
